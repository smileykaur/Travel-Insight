{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Topic Modelling, Aspect Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from gensim.models import LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "\n",
    "import os, re, operator, warnings\n",
    "warnings.filterwarnings('ignore')  # Let's not pay heed to them right now\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/smiley/PycharmProjects/Travel_Insights/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text Corpus\n",
    "text_corpus=pd.read_csv(\"../data/text_corpus.csv\")\n",
    "\n",
    "# NLP Processed, TODO: move this to 1st step of data\n",
    "nlp_df=pd.read_csv(\"../data/nlp_processed_corpus.csv\")\n",
    "\n",
    "\n",
    "comments_df=pd.read_csv(\"../data/grouped_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8h6aao</td>\n",
       "      <td>Wife and I hate big social events and love tra...</td>\n",
       "      <td>Wife and I hate big social events and love tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95l2e6</td>\n",
       "      <td>The exact moment I took a step too close to th...</td>\n",
       "      <td>The exact moment I took a step too close to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8yj2tg</td>\n",
       "      <td>Wandering around Kyoto at night</td>\n",
       "      <td>Wandering around Kyoto at nightKyoto is amazin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8i4939</td>\n",
       "      <td>I heard this place had stunning views but I ju...</td>\n",
       "      <td>I heard this place had stunning views but I ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85awza</td>\n",
       "      <td>Went to the top of the Eiffel Tower and there ...</td>\n",
       "      <td>Went to the top of the Eiffel Tower and there ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id                                   submission_title  \\\n",
       "0        8h6aao  Wife and I hate big social events and love tra...   \n",
       "1        95l2e6  The exact moment I took a step too close to th...   \n",
       "2        8yj2tg                    Wandering around Kyoto at night   \n",
       "3        8i4939  I heard this place had stunning views but I ju...   \n",
       "4        85awza  Went to the top of the Eiffel Tower and there ...   \n",
       "\n",
       "                                                text  \n",
       "0  Wife and I hate big social events and love tra...  \n",
       "1  The exact moment I took a step too close to th...  \n",
       "2  Wandering around Kyoto at nightKyoto is amazin...  \n",
       "3  I heard this place had stunning views but I ju...  \n",
       "4  Went to the top of the Eiffel Tower and there ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning and merge data earlier\n",
    "text_corpus.head()  #grouped_by_submission dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"wife hate social event traveling. rather normal wedding traveled switzerland vow private. dayit's strange swear wife congratulation awful stacked crate flower arrangement top.i guess type antisocial show wedding. broke sentence middle helped delivery. great. box eh? hope bridesmaids... couple? fall cliff? stacked stack stacks? congratulation marriage bonerdude username out? outside box smart. wedding garbage waste money. traveling valuable. marriage legal country? relate joke brother terrible social anxiety sil ended canceling wedding courthouse instead. idea complementing sentence structure joke figure liner charm it! try. you. sturdy. built. pounding. handle lose grip. stack down. congratulation mr. mrs. bonerdude guest wedding. emptying wine crate chatting away. drunk uncle aunt grab quick dance. party. overflow bonerdude taking dude name keeping maiden name xxsniperxx? upvote upvotes moment modern they're hyphenating. mrs. xxdude sniperxx xxsniperxx redditor saving born resist urge\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#should be a cleaned version\n",
    "text_corpus['text'][0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_corpus['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments_id</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dyhi85d</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>It's strange, but I swear you and your new wif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dyhfwly</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>That is really nice, u/bonerdude420Is she taki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dyhdpyi</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>Taken near Murren in Oct 2016. Only people the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dyhi3r7</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>Way back in 2002 before destination weddings w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dyhfqeg</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>We did something very similar. Got [married ou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comments_id submission_id                                            comment\n",
       "0     dyhi85d        8h6aao  It's strange, but I swear you and your new wif...\n",
       "1     dyhfwly        8h6aao  That is really nice, u/bonerdude420Is she taki...\n",
       "2     dyhdpyi        8h6aao  Taken near Murren in Oct 2016. Only people the...\n",
       "3     dyhi3r7        8h6aao  Way back in 2002 before destination weddings w...\n",
       "4     dyhfqeg        8h6aao  We did something very similar. Got [married ou..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head()  #grouped_by_comments dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's strange, but I swear you and your new wife (congratulations btw), look an awful lot like stacked crates with flower arrangements on top.I guess she's my type,Or they are so antisocial that they didn't show up for their own wedding.,The way you broke up the sentence in the middle really helped with the delivery. That was great.,Nice box, eh?,I hope she’s not the mountain :/,You should have seen the bridesmaids...,Where is the couple? Did they fall off the cliff?,stacked with stacks on stacks?,congratulations on your marriage bonerdude420. username may or may not check out?,Some people just can’t think outside the box,So smart.  Wedding are such a garbage waste of money.  Traveling is so much more valuable. Is the marriage legal in your home country?,I can relate to this,You joke but my brother has really terrible social anxiety and he and my now SIL ended up canceling their wedding and just went to the courthouse instead.,I really like this idea of complementing the good sentence s\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df['comment'][0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stopWords = stopwords.words('english')\n",
    "lt = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Topic modelling data cleaning \n",
    "\n",
    "def tokenize(text, lower=True):\n",
    "    if lower:\n",
    "        return [token.strip().lower() for token in text.split()]\n",
    "    else:\n",
    "        return [token.strip() for token in text.split()]\n",
    "\n",
    "def remove_stop_words( text):\n",
    "    return [word for word in text if word not in stopWords]\n",
    "\n",
    "def lemmatize(text):\n",
    "    return [(lt.lemmatize(x)) for x in text]\n",
    "\n",
    "def clean_text(text,token=True):\n",
    "    # regex to remove URL from string\n",
    "    text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "    # regex to remove special characters from string except [: ? ! . , ']\n",
    "    text = re.sub(r\"[^a-zA-Z?!'.]\", \" \", text)\n",
    "\n",
    "    tokenized_text = tokenize(text)\n",
    "    lemmatized_text = lemmatize(tokenized_text)\n",
    "    cleaned_text =remove_stop_words(lemmatized_text)\n",
    "    \n",
    "    if token:\n",
    "        return [val for val in cleaned_text]\n",
    "    else:\n",
    "        return ' '.join(token for token in cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find words frequency.\n",
    "all_words = []\n",
    "for i in text_corpus['text']:\n",
    "    all_words.extend(i.split())\n",
    "fdist = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of common words\n",
    "word_list=[x[0] for x in fdist.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords.extend([ \"[REMOVED]\", \"[deleted]\", 'lol', 'wtf', 'ha' , 'wa', \"i'd\", \"btw\" ])\n",
    "\n",
    "stopWords.extend(word_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning text_corpus\n",
    "text_corpus['text']=text_corpus['text'].apply(lambda x: clean_text(x, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8h6aao</td>\n",
       "      <td>Wife and I hate big social events and love tra...</td>\n",
       "      <td>wife hate social event traveling. rather norma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95l2e6</td>\n",
       "      <td>The exact moment I took a step too close to th...</td>\n",
       "      <td>exact moment step close border korea push butt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8yj2tg</td>\n",
       "      <td>Wandering around Kyoto at night</td>\n",
       "      <td>wandering kyoto nightkyoto amazing. moment urb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8i4939</td>\n",
       "      <td>I heard this place had stunning views but I ju...</td>\n",
       "      <td>stunning prepared this. jaw dropped. lauterbru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85awza</td>\n",
       "      <td>Went to the top of the Eiffel Tower and there ...</td>\n",
       "      <td>top eiffel tower happened rainbow parislooks t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id                                   submission_title  \\\n",
       "0        8h6aao  Wife and I hate big social events and love tra...   \n",
       "1        95l2e6  The exact moment I took a step too close to th...   \n",
       "2        8yj2tg                    Wandering around Kyoto at night   \n",
       "3        8i4939  I heard this place had stunning views but I ju...   \n",
       "4        85awza  Went to the top of the Eiffel Tower and there ...   \n",
       "\n",
       "                                                text  \n",
       "0  wife hate social event traveling. rather norma...  \n",
       "1  exact moment step close border korea push butt...  \n",
       "2  wandering kyoto nightkyoto amazing. moment urb...  \n",
       "3  stunning prepared this. jaw dropped. lauterbru...  \n",
       "4  top eiffel tower happened rainbow parislooks t...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning comments corpus\n",
    "comments_df['tokens']= comments_df['comment'].apply(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments_id</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dyhi85d</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>It's strange, but I swear you and your new wif...</td>\n",
       "      <td>[strange, swear, wife, congratulation, awful, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dyhfwly</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>That is really nice, u/bonerdude420Is she taki...</td>\n",
       "      <td>[bonerdude, taking, dude, name, keeping, maide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dyhdpyi</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>Taken near Murren in Oct 2016. Only people the...</td>\n",
       "      <td>[taken, murren, oct, photographer, artist, cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dyhi3r7</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>Way back in 2002 before destination weddings w...</td>\n",
       "      <td>[destination, wedding, chose, wedding, date, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dyhfqeg</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>We did something very similar. Got [married ou...</td>\n",
       "      <td>[similar., married, outdoors, sea, finland, br...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comments_id submission_id  \\\n",
       "0     dyhi85d        8h6aao   \n",
       "1     dyhfwly        8h6aao   \n",
       "2     dyhdpyi        8h6aao   \n",
       "3     dyhi3r7        8h6aao   \n",
       "4     dyhfqeg        8h6aao   \n",
       "\n",
       "                                             comment  \\\n",
       "0  It's strange, but I swear you and your new wif...   \n",
       "1  That is really nice, u/bonerdude420Is she taki...   \n",
       "2  Taken near Murren in Oct 2016. Only people the...   \n",
       "3  Way back in 2002 before destination weddings w...   \n",
       "4  We did something very similar. Got [married ou...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [strange, swear, wife, congratulation, awful, ...  \n",
       "1  [bonerdude, taking, dude, name, keeping, maide...  \n",
       "2  [taken, murren, oct, photographer, artist, cel...  \n",
       "3  [destination, wedding, chose, wedding, date, b...  \n",
       "4  [similar., married, outdoors, sea, finland, br...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 . Identifying Aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule based matching with \"Spacy Matcher\"\n",
    "Defining Tagging rules https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3888732/#sec2\n",
    "\n",
    "Rules based on part of speech tags\n",
    "\n",
    "Pattern first word second word third word\n",
    "\n",
    "    Pattern 1 NN amod NNP\n",
    "    Pattern 2 NN NN\n",
    "    Pattern 3 NN GPE\n",
    "    Pattern 4 VBG ROOT VBG\n",
    "   \n",
    "   \n",
    "Example sentences:\n",
    "\n",
    "1. ('kayak', 'NN', 'amod', 'island', 'NN'),\n",
    "2. ('climbing', 'NN', 'xcomp', 'sobbing', 'VBG'),\n",
    "3. ('honeymoon', 'VBP', 'ROOT', 'honeymoon', 'VBP'),\n",
    "4. ('snorkeling', 'VBG', 'ROOT', 'snorkeling', 'VBG'),\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Pattern\n",
    "pattern1 = [{'TAG': 'NN' }, {'DEP': 'amod'}, {'TAG': 'NNP' }]\n",
    "pattern2 = [{'TAG': 'NN' }, {'TAG': 'NN' }]\n",
    "pattern3 = [{'TAG': 'NN' } , {'ENT_TYPE':'GPE'}]\n",
    "pattern4 = [{'TAG': 'VBG' }, {'DEP' : 'ROOT'}, {'TAG':'VBG' }]\n",
    "\n",
    "# This is a list of names of patterns used. If a new pattern is created add the name of pattern to this list\n",
    "patternsP=[pattern1, pattern2, pattern3, pattern4]\n",
    "\n",
    "# add all the patterns created above\n",
    "for i in patternsP: \n",
    "    matcher.add(str(i),None,i) # adds pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate dictionary of most frequest phrases\n",
    "\n",
    "\n",
    "# reason why I am adding a set is because there are multiple patterns which are matching\n",
    "sent=set()\n",
    "\n",
    "def generate_frequent_phrases(text,submission_id):\n",
    "    \"\"\"\n",
    "    text - the text to be analyzed, \n",
    "    \"\"\"\n",
    "    doc=nlp(text)\n",
    "    phrases=dict() \n",
    "    # generate matches\n",
    "    matches = matcher(doc) \n",
    "    \n",
    "    # capturing text from matcher\n",
    "    for match_id, start, end in matches:      \n",
    "        sent.add(doc[start:end])\n",
    "    \n",
    "    phrases[submission_id]=sent\n",
    "    \n",
    "    return phrases\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate Phrases\n",
    "text_corpus['phrases']=text_corpus.apply(lambda x: generate_frequent_phrases(x.text, x.submission_id), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Phrases with submission id as key and values as a set of phrases\n",
    "#phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>text</th>\n",
       "      <th>phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8h6aao</td>\n",
       "      <td>Wife and I hate big social events and love tra...</td>\n",
       "      <td>wife hate social event traveling. rather norma...</td>\n",
       "      <td>{'8h6aao': {(strip, life), (shrine, market), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95l2e6</td>\n",
       "      <td>The exact moment I took a step too close to th...</td>\n",
       "      <td>exact moment step close border korea push butt...</td>\n",
       "      <td>{'95l2e6': {(strip, life), (shrine, market), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8yj2tg</td>\n",
       "      <td>Wandering around Kyoto at night</td>\n",
       "      <td>wandering kyoto nightkyoto amazing. moment urb...</td>\n",
       "      <td>{'8yj2tg': {(strip, life), (shrine, market), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8i4939</td>\n",
       "      <td>I heard this place had stunning views but I ju...</td>\n",
       "      <td>stunning prepared this. jaw dropped. lauterbru...</td>\n",
       "      <td>{'8i4939': {(strip, life), (shrine, market), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85awza</td>\n",
       "      <td>Went to the top of the Eiffel Tower and there ...</td>\n",
       "      <td>top eiffel tower happened rainbow parislooks t...</td>\n",
       "      <td>{'85awza': {(strip, life), (shrine, market), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id                                   submission_title  \\\n",
       "0        8h6aao  Wife and I hate big social events and love tra...   \n",
       "1        95l2e6  The exact moment I took a step too close to th...   \n",
       "2        8yj2tg                    Wandering around Kyoto at night   \n",
       "3        8i4939  I heard this place had stunning views but I ju...   \n",
       "4        85awza  Went to the top of the Eiffel Tower and there ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  wife hate social event traveling. rather norma...   \n",
       "1  exact moment step close border korea push butt...   \n",
       "2  wandering kyoto nightkyoto amazing. moment urb...   \n",
       "3  stunning prepared this. jaw dropped. lauterbru...   \n",
       "4  top eiffel tower happened rainbow parislooks t...   \n",
       "\n",
       "                                             phrases  \n",
       "0  {'8h6aao': {(strip, life), (shrine, market), (...  \n",
       "1  {'95l2e6': {(strip, life), (shrine, market), (...  \n",
       "2  {'8yj2tg': {(strip, life), (shrine, market), (...  \n",
       "3  {'8i4939': {(strip, life), (shrine, market), (...  \n",
       "4  {'85awza': {(strip, life), (shrine, market), (...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to check patterns of sentences manually\n",
    "#doc=nlp(text_corpus['text'][1]) \n",
    "#[(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4 : Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments_id</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dyhi85d</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>It's strange, but I swear you and your new wif...</td>\n",
       "      <td>[strange, swear, wife, congratulation, awful, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dyhfwly</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>That is really nice, u/bonerdude420Is she taki...</td>\n",
       "      <td>[bonerdude, taking, dude, name, keeping, maide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dyhdpyi</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>Taken near Murren in Oct 2016. Only people the...</td>\n",
       "      <td>[taken, murren, oct, photographer, artist, cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dyhi3r7</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>Way back in 2002 before destination weddings w...</td>\n",
       "      <td>[destination, wedding, chose, wedding, date, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dyhfqeg</td>\n",
       "      <td>8h6aao</td>\n",
       "      <td>We did something very similar. Got [married ou...</td>\n",
       "      <td>[similar., married, outdoors, sea, finland, br...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comments_id submission_id  \\\n",
       "0     dyhi85d        8h6aao   \n",
       "1     dyhfwly        8h6aao   \n",
       "2     dyhdpyi        8h6aao   \n",
       "3     dyhi3r7        8h6aao   \n",
       "4     dyhfqeg        8h6aao   \n",
       "\n",
       "                                             comment  \\\n",
       "0  It's strange, but I swear you and your new wif...   \n",
       "1  That is really nice, u/bonerdude420Is she taki...   \n",
       "2  Taken near Murren in Oct 2016. Only people the...   \n",
       "3  Way back in 2002 before destination weddings w...   \n",
       "4  We did something very similar. Got [married ou...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [strange, swear, wife, congratulation, awful, ...  \n",
       "1  [bonerdude, taking, dude, name, keeping, maide...  \n",
       "2  [taken, murren, oct, photographer, artist, cel...  \n",
       "3  [destination, wedding, chose, wedding, date, b...  \n",
       "4  [similar., married, outdoors, sea, finland, br...  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(comments_df.query('submission_id==\"8h6aao\"')['tokens'])\n",
    "texts = [bigram[line] for line in comments_df.query('submission_id==\"8h6aao\"')['tokens']]\n",
    "\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LSI \n",
    "\n",
    "LSI stands for Latent Semantic Indeixing - it is a popular information retreival method which works by decomposing the original matrix of words to maintain key topics. Gensim's implementation uses an SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsimodel = LsiModel(corpus=corpus, num_topics=5, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.696*\"wedding\" + 0.153*\"married\" + 0.132*\"ceremony\" + 0.131*\"wife\" + 0.113*\"super\" + 0.102*\"dinner\" + 0.093*\"wedding.\" + 0.091*\"eloped\" + 0.087*\"care\" + 0.081*\"together\"'),\n",
       " (1,\n",
       "  '-0.368*\"name\" + -0.230*\"creamyanus\" + -0.182*\"show\" + -0.163*\"future.\" + -0.162*\"tradition\" + -0.141*\"change\" + -0.136*\"cooler\" + -0.110*\"wedding.\" + -0.094*\"amount\" + -0.091*\"large\"')]"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsimodel.show_topics(num_topics=2)  # Showing only the top 2 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDP \n",
    "\n",
    "HDP, the Hierarchical Dirichlet Process is an unsupervised topic model which figures out the number of topics on it's own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdpmodel = HdpModel(corpus=corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  \"0.006*wedding + 0.004*others + 0.004*stranger + 0.003*you...it's + 0.003*week. + 0.003*angry + 0.003*lose + 0.003*stayed + 0.003*remove + 0.003*officiated + 0.003*station. + 0.003*sensible + 0.003*sticking + 0.002*mentality + 0.002*this! + 0.002*referred + 0.002*expect.better + 0.002*ha! + 0.002*portfolio + 0.002*soooooo\"),\n",
       " (1,\n",
       "  \"0.005*wedding + 0.004*photo...the + 0.004*eloping. + 0.004*one.sounds + 0.004*stacks?it's + 0.004*candles.my + 0.003*community + 0.003*nights. + 0.003*hurt + 0.003*sea + 0.003*poor + 0.003*detail + 0.003*attitude + 0.003*whether + 0.003*weird + 0.003*her. + 0.003*married + 0.003*grab + 0.002*accept + 0.002*dinner\")]"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdpmodel.show_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "LDA, or Latent Dirichlet Allocation is arguably the most famous topic modelling algorithm out there. Out here we create a simple topic model with 10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus=corpus, num_topics=5, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: nan*\"slightest\" + nan*\"so...\" + nan*\"riddled\" + nan*\"rule\" + nan*\"sense\" + nan*\"sig\" + nan*\"it'll\" + nan*\"trough\" + nan*\"std\" + nan*\"ha!\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: nan*\"slightest\" + nan*\"so...\" + nan*\"riddled\" + nan*\"rule\" + nan*\"sense\" + nan*\"sig\" + nan*\"it'll\" + nan*\"trough\" + nan*\"std\" + nan*\"ha!\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: nan*\"slightest\" + nan*\"so...\" + nan*\"riddled\" + nan*\"rule\" + nan*\"sense\" + nan*\"sig\" + nan*\"it'll\" + nan*\"trough\" + nan*\"std\" + nan*\"ha!\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: nan*\"slightest\" + nan*\"so...\" + nan*\"riddled\" + nan*\"rule\" + nan*\"sense\" + nan*\"sig\" + nan*\"it'll\" + nan*\"trough\" + nan*\"std\" + nan*\"ha!\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: nan*\"slightest\" + nan*\"so...\" + nan*\"riddled\" + nan*\"rule\" + nan*\"sense\" + nan*\"sig\" + nan*\"it'll\" + nan*\"trough\" + nan*\"std\" + nan*\"ha!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD & Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 1000)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopWords, \n",
    "                             max_features= 1000, # keep top 1000 terms \n",
    "                             max_df = 0.5, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(comments_df.query('submission_id==\"8h6aao\"')['comment'])\n",
    "\n",
    "X.shape # check shape of the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=5, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X)\n",
    "\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "wedding\n",
      "friends\n",
      "us\n",
      "watching\n",
      "weddings\n",
      "married\n",
      "share\n",
      " \n",
      "Topic 1: \n",
      "watching\n",
      "share\n",
      "media\n",
      "social\n",
      "moment\n",
      "giant\n",
      "happen\n",
      " \n",
      "Topic 2: \n",
      "shoot\n",
      "honestly\n",
      "watching\n",
      "fun\n",
      "selective\n",
      "conflict\n",
      "giants\n",
      " \n",
      "Topic 3: \n",
      "sarcasm\n",
      "drama\n",
      "queen\n",
      "fuck\n",
      "taking\n",
      "seriously\n",
      "sarcastic\n",
      " \n",
      "Topic 4: \n",
      "makeup\n",
      "done\n",
      "marrying\n",
      "vain\n",
      "women\n",
      "professionally\n",
      "invite\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Train model and save the model results.\n",
    "\n",
    "#pickle API for serializing standard Python objects\n",
    "import pickle\n",
    "pickle.dump(ldamodel, open(\"PATH/model/lda.pickle\", \"wb\"))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "\n",
    "#joblib API for efficiently serializing Python objects with NumPy arrays.\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(ldamodel, 'ldamodel.pkl')\n",
    "loaded_model=open('ldamodel.pkl','rb')\n",
    "topic_model = joblib.load(loaded_model)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text_tokens= text.split()\n",
    "    text_tokens=[i.strip().lower() for i in text_tokens]\n",
    "    text_tokens=[[i for i in text_tokens if i not in stopWords]]\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pyLDAvis\n",
    "\n",
    "Using this library to visualise our topic models. What make pyLDAvis special is that it gives a UI to user (top right in below diagram) that allow to set the value of lambda which lambda values 0 - Negative and 1- Positive. One can see from best to worst topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Topic Model - LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_topic(df):\n",
    "    bigram = gensim.models.Phrases(df)\n",
    "    #texts\n",
    "    texts = [bigram[line] for line in df]\n",
    "    \n",
    "    dictionary = Dictionary(texts)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    lsimodel = LsiModel(corpus=corpus, num_topics=5, id2word=dictionary)\n",
    "    results=lsimodel.print_topics()\n",
    "    \n",
    "    topics=list()\n",
    "    for i, topic in enumerate(results):\n",
    "        out=[]\n",
    "        for word in topic[1].split('+'): \n",
    "            res=word.split('*')\n",
    "            out.append(( float(res[0]), res[1]))\n",
    "        sorted_list=sorted(out, key=lambda x: x[0], reverse=True)\n",
    "        topics.extend([i[1].strip()[1:-1]for i in sorted_list[:5]])\n",
    "    return ','.join([i for i in topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_dict={\"submission_id\":[],\"topics\":[]}\n",
    "topics_dict[\"submission_id\"]=list(text_corpus.submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sub_id in topics_dict[\"submission_id\"]:\n",
    "    topics_dict[\"topics\"].append(generate_topic(comments_df.query('submission_id==\"{0}\"'.format(sub_id))['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_df=pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8h6aao</td>\n",
       "      <td>wedding,married,ceremony,wife,super,name,cream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95l2e6</td>\n",
       "      <td>nk,korean,guard,sk,border,called,auto,?,team,r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8yj2tg</td>\n",
       "      <td>kyoto,safe,kyoto.,tokyo,japanese,us.,president...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id                                             topics\n",
       "0        8h6aao  wedding,married,ceremony,wife,super,name,cream...\n",
       "1        95l2e6  nk,korean,guard,sk,border,called,auto,?,team,r...\n",
       "2        8yj2tg  kyoto,safe,kyoto.,tokyo,japanese,us.,president..."
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['wedding', 'married', 'ceremony', 'wife', 'super']\n",
      "2 ['large', 'amount', 'wedding.', 'cooler', 'change']\n",
      "3 ['paperwork', 'legal', 'care', 'piece_paper', 'italy']\n",
      "4 ['wedding', 'parent', 'saying', 'married', 'fight']\n",
      "5 ['wedding.', 'legal', 'australia', 'similar', 'spain']\n"
     ]
    }
   ],
   "source": [
    "cnt=1\n",
    "for i in range(0,len(a),5):    \n",
    "    print(cnt, a[i:i+5])\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_df.to_csv(\"./topics_by_submissionid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chiku's Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Required libraries.\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pickle\n",
    "import gensim\n",
    "import time\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stopWords = set(stopwords.words('english'))\n",
    "lt = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up tokenizer.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Set up stop words.\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Set up stemmer.\n",
    "p_stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LDA_Clean_Text(review_main_text_list):\n",
    "    # List for the review texts that are tokenized, stop word deleted and stemmed.\n",
    "    cleaned_up_review_list = []\n",
    "    # For every review.\n",
    "    for document in review_main_text_list:\n",
    "        # Use the lowercase of all letters.\n",
    "        raw = document.lower()\n",
    "        # Tokenization\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # Manually deleting stop words.\n",
    "        j = 0\n",
    "\n",
    "        while j < len(tokens):\n",
    "            if tokens[j] in stop:\n",
    "                del tokens[j]\n",
    "            else:\n",
    "                j += 1\n",
    "        # Stem each word.\n",
    "        #cleaned_text = [p_stemmer.stem(i) for i in tokens]\n",
    "        cleaned_text = [i for i in tokens]\n",
    "        # Add cleaned review text to list.\n",
    "        cleaned_up_review_list.append(cleaned_text)\n",
    "\n",
    "    return cleaned_up_review_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LDA():\n",
    "    \"\"\"\n",
    "    Generate Topics from the reviews corpora\n",
    "    :param reviews: Reviews is a dataset of text reviews for doctors based on location_id\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Creating a list of all reviews\n",
    "    #review_main_text_list = [review for review in df.query('submission_id==\"8h6aao\"')['comment']]\n",
    "    review_main_text_list = [review for review in df.query('submission_id==\"8h6aao\"')['comment']]\n",
    "\n",
    "    # Step 2: Cleaning the reviews text\n",
    "    cleaned_up_review_list = LDA_Clean_Text(review_main_text_list)\n",
    "\n",
    "    # Checking word frequency with NLTK\n",
    "    all_words = []\n",
    "    for i in cleaned_up_review_list:\n",
    "        for j in i:\n",
    "            all_words.append(j)\n",
    "\n",
    "    # frequency distribution of words\n",
    "    fdist = nltk.FreqDist(all_words)\n",
    "\n",
    "    # Print the most common words.\n",
    "    print(fdist.most_common())\n",
    "\n",
    "    # Step3: Filter out the words that we want to ignore.\n",
    "    #cleaned_up_review_list2 = LDA_words_to_ignore(cleaned_up_review_list)\n",
    "\n",
    "    # Generate dictionary and corpus from the remaining words.\n",
    "    dictionary = gensim.corpora.Dictionary(cleaned_up_review_list)\n",
    "    corpus = [dictionary.doc2bow(word) for word in cleaned_up_review_list]\n",
    "\n",
    "    # Step 4: Generate LDA Model\n",
    "    # Setting parameters for LDA:\n",
    "    no_of_topics = 15\n",
    "    passes_in = 100\n",
    "    ldamodel = Train_LDA(corpus, dictionary, no_of_topics, passes_in)\n",
    "\n",
    "    # Step 5: Check resulting topics.\n",
    "    topic_list = ldamodel.print_topics(num_topics=no_of_topics, num_words=25)\n",
    "\n",
    "    for index, i in enumerate(topic_list):\n",
    "        str1 = str(i[1])\n",
    "        for c in \"0123456789+*\\\".\":\n",
    "            str1 = str1.replace(c, \"\")\n",
    "        str1 = str1.replace(\"  \", \" \")\n",
    "        print(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Train_LDA(corpus, dictionary, no_of_topics=15, passes_in=10):\n",
    "    # Step 4: Train LDA model.\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=no_of_topics, id2word=dictionary, passes=passes_in,\n",
    "                                               alpha='asymmetric')\n",
    "    # Saving the results\n",
    "    #pickle.dump(ldamodel, open(\"../model/lda3.pickle\", \"wb\"))\n",
    "    #pickle.dump(dictionary, open(\"../model/dictionary3.pickle\", \"wb\"))\n",
    "    #pickle.dump(corpus, open(\"../model/corpus3.pickle\", \"wb\"))\n",
    "\n",
    "    return ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords.extend([[REMOVED], [deleted], 'lol', 'wtf', 'ha' , 'wa', \"i'd\", ])\n",
    "\n",
    "stopWords.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CleaningText:\n",
    "    def tokenize(self, text, lower=True):\n",
    "        if lower:\n",
    "            return [token.strip().lower() for token in text.split()]\n",
    "        else:\n",
    "            return [token.strip() for token in text.split()]\n",
    "\n",
    "    def remove_stop_words(self, text):\n",
    "        return [word for word in text if word not in stopWords]\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        return [(lt.lemmatize(x)) for x in text]\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # regex to remove URL from string\n",
    "        text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "        # regex to remove special characters from string except [: ? ! . , ']\n",
    "        text = re.sub(r\"[^a-zA-Z?!'.]\", \" \", text)\n",
    "\n",
    "        tokenized_text = self.tokenize(text, True)\n",
    "        lemmatized_text = self.lemmatize(tokenized_text)\n",
    "        cleaned_text = self.remove_stop_words(lemmatized_text)\n",
    "\n",
    "        return ' '.join(token for token in cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/aggregated_data/group_by_comments.csv\")\n",
    "# 8h6aao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8h6aao'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.submission_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data():\n",
    "    # Step 1: Clean the input data\n",
    "    # Loading the dataset\n",
    "    df = pd.read_csv(\"../data/aggregated_data/group_by_comments.csv\")\n",
    "    print(df.columns)\n",
    "    # Dropping unwanted column\n",
    "    #data.drop(data.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "    # Taking subset of columns\n",
    "    #df = data[['Location', 'Description', 'Rating']]\n",
    "\n",
    "    # drop na values\n",
    "    df = df.dropna()\n",
    "    # resetting index after deleting NA rows\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Call the LDA Function\n",
    "    LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
